{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d5b987",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f98035",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99655abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7dd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b41c2f",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160abe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    comments = pd.read_csv('/toxic_comments.csv') \n",
    "except:\n",
    "    comments = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(comments.head(5))\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "comments.info()\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "print('Дубликатов -', comments.duplicated().sum())\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "print('Пропусков:')\n",
    "display(comments.isna().sum())\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "print('Соотношение в целевом признаке:')\n",
    "display(comments.toxic.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742e662",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Выводы\n",
    "<li>В таблице 159 292 объектa. Пропусков нет, явных дубликатов нет</li>\n",
    "<li>Тексты комментариев на английском, есть лишние знаки типа \"\\nMore\\n</li>\n",
    "<li>Наблюдаем явный дизбаланс классов</li>\n",
    "<li>Необходимо избавиться от столбца Unnamed, так как он фактически дублирует индексы</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для очищения текстов постов:\n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)   \n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#очищаю тексты:\n",
    "comments['text'] = comments['text'].apply(clear_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ввожу функцию РОS-тэгирования слов:\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,               #прилагательное\n",
    "                \"N\": wordnet.NOUN,              #существительное\n",
    "                \"V\": wordnet.VERB,              #глагол\n",
    "                \"R\": wordnet.ADV                #наречие\n",
    "               }  \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#ввожу функцию леммализации тектов постов:\n",
    "def lemm_text(text):\n",
    "    text = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a89983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#леммализирую тексты постов:\n",
    "comments['text'] = comments['text'].apply(lemm_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделяю выборки в соотношении 60/20/20:\n",
    "features = comments.drop(['toxic'], axis=1) \n",
    "target = comments.toxic\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=.4, \n",
    "                                                                              random_state=34)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=.5,\n",
    "                                                                            random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#смотрю размеры выборок:\n",
    "for i in [features_train, target_train, features_valid, target_valid, features_test, target_test]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#смотрю соотношение 1/0 в выборках на примере target_train:\n",
    "indices_1 = [i for i,x in enumerate(target_train) if x == 1]\n",
    "count_1 = len(indices_1)\n",
    "\n",
    "indices_0 = [i for i,x in enumerate(target_train) if x == 0]\n",
    "count_0 = len(indices_0)\n",
    "\n",
    "print('Доля значений 1 в тренировочной выборке:', len(indices_1) / (len(indices_1) + len(indices_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac13af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#уменьшаю кол-во 0 в выборках train:\n",
    "\n",
    "comments_train = comments.iloc[target_train.index]\n",
    "target_train_0 = comments_train[comments_train['toxic'] == 0]['toxic']\n",
    "target_train_1 = comments_train[comments_train['toxic'] == 1]['toxic']\n",
    "\n",
    "\n",
    "target_train_0_resample = target_train_0.sample(target_train_1.shape[0], random_state=12345)\n",
    "target_train_resample = pd.concat([target_train_0_resample, target_train_1])\n",
    "\n",
    "features_train_resample = comments.iloc[target_train_resample.index]\n",
    "\n",
    "features_train_resample, target_train_resample = shuffle(features_train_resample,\n",
    "                                                         target_train_resample,\n",
    "                                                         random_state=42)\n",
    "\n",
    "features_train_resample = features_train_resample.text \n",
    "\n",
    "print('Соотношение 1/0 в тренировочной выборке:')\n",
    "print(target_train_resample.value_counts(normalize=True))\n",
    "print()\n",
    "print(features_train_resample.shape)\n",
    "print(target_train_resample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc41ca",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Выводы\n",
    "<li>Удалил ненужный столбец</li>\n",
    "<li>Очистил тексты комментариев от ненужных знаков, леммализировал, убрал стоп-слова</li>\n",
    "<li>Сбалансировал данные в целевом признаке</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8ce04",
   "metadata": {},
   "source": [
    "\n",
    "## Обучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec868b98",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Логистическая регрессия\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f07fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178857b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english', sublinear_tf=True)), \n",
    "                     (\"lr\", LogisticRegression())])\n",
    "    \n",
    "parameters = {'lr__solver': ('liblinear', 'saga','newton-cg', 'lbfgs'),\n",
    "              'lr__C': (.1, 1, 5, 10),\n",
    "              'lr__random_state': ([42]),\n",
    "              'lr__max_iter': ([200])}\n",
    "\n",
    "gscv_log = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv_log.fit(features_train, target_train)\n",
    "\n",
    "mts = gscv_log.cv_results_['mean_test_score']\n",
    "lr_train_f1 = max(mts)\n",
    "\n",
    "print('F1 логистической регрессии =', round(lr_train_f1,2))\n",
    "print('при параметрах', gscv_log.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv_log.predict(features_valid.text)\n",
    "lr_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 логистической регрессии на валидации =', round(lr_valid_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4098e",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Дерево решений\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english')), \n",
    "                     (\"dtc\", DecisionTreeClassifier())])\n",
    "    \n",
    "parameters = {'dtc__max_depth': ([x for x in range(1, 25)]),\n",
    "              'dtc__random_state': ([42]), \n",
    "              'dtc__class_weight': (['balanced'])}\n",
    "\n",
    "gscv_tree = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv_tree.fit(features_train_resample, target_train_resample)\n",
    "\n",
    "mts = gscv_tree.cv_results_['mean_test_score']\n",
    "dtc_train_f1 = max(mts)\n",
    "\n",
    "print('F1 дерева решений =', round(dtc_train_f1,2))\n",
    "print('при параметрах', gscv_tree.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv_tree.predict(features_valid.text)\n",
    "dtc_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 дерева решений на валидации =', round(dtc_valid_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc5615",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "CatBoostClassifier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english')), \n",
    "                     (\"cbc\", CatBoostClassifier())])\n",
    "    \n",
    "parameters = {'cbc__verbose': ([False]),\n",
    "              'cbc__iterations': ([200]),\n",
    "              'cbc__class_weights':([(1, 1), (1, 11)])} #вот вообще не уверена, что class_weights тут нужен\n",
    "\n",
    "gscv_cat = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv_cat.fit(features_train_resample, target_train_resample)\n",
    "\n",
    "mts = gscv_cat.cv_results_['mean_test_score']\n",
    "cbc_train_f1 = max(mts)\n",
    "\n",
    "print('F1 CatBoostClassifier =', round(cbc_train_f1,2))\n",
    "print('при параметрах', gscv_cat.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv_cat.predict(features_valid.text)\n",
    "cbc_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 CatBoostClassifier на валидации =', round(cbc_valid_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79118e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "RandomForestClassifier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english')), \n",
    "                     (\"rfc\", RandomForestClassifier())])\n",
    "    \n",
    "parameters = {'rfc__n_estimators': ([x for x in range(10, 30)]),\n",
    "              'rfc__random_state': ([42]),\n",
    "              'rfc__max_depth': ([x for x in range(1, 10)]),\n",
    "              'rfc__criterion': (['entropy']),\n",
    "              'rfc__class_weight': (['balanced'])}\n",
    "\n",
    "gscv_rfc = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv_rfc.fit(features_train_resample, target_train_resample)\n",
    "\n",
    "mts = gscv_rfc.cv_results_['mean_test_score']\n",
    "rfc_train_f1 = max(mts)\n",
    "\n",
    "print('F1 случайного леса =', round(rfc_train_f1,2))\n",
    "print('при параметрах', gscv_rfc.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv_rfc.predict(features_valid.text)\n",
    "rfc_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 случайного леса на валидации =', round(rfc_valid_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed613c68",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "SGDClassifier\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c230e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english')), \n",
    "                     (\"clf\", SGDClassifier())])\n",
    "    \n",
    "parameters = {'clf__loss': ('hinge', 'log', 'modified_huber'),\n",
    "              'clf__learning_rate': ('constant', 'optimal', 'invscaling', 'adaptive'),\n",
    "              'clf__eta0': (.01, .05, .1, .5),\n",
    "              'clf__random_state': ([42]),\n",
    "              'clf__class_weight': (['balanced'])}\n",
    "\n",
    "gscv_sgd = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv_sgd.fit(features_train_resample, target_train_resample)\n",
    "\n",
    "mts = gscv_sgd.cv_results_['mean_test_score']\n",
    "sgdc_train_f1 = max(mts)\n",
    "\n",
    "print('F1 SGDClassifier =', round(sgdc_train_f1,2))\n",
    "print('при параметрах', gscv_sgd.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv_sgd.predict(features_valid.text)\n",
    "sgdc_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 SGDClassifier на валидации =', round(sgdc_valid_f1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = ['LogisticRegression',\n",
    "         'DecisionTreeClassifier',\n",
    "         'CatBoostClassifier',\n",
    "         'RandomForestClassifier',\n",
    "         'SGDClassifier'\n",
    "        ]\n",
    "\n",
    "data = {'F1 на обучающей выборке': [lr_train_f1,\n",
    "                                    dtc_train_f1,\n",
    "                                    cbc_train_f1,\n",
    "                                    rfc_train_f1,\n",
    "                                    sgdc_train_f1],\n",
    "        \n",
    "        'F1 на валидационной выборке': [lr_valid_f1,\n",
    "                                        dtc_valid_f1,\n",
    "                                        cbc_valid_f1,\n",
    "                                        rfc_valid_f1,\n",
    "                                        sgdc_valid_f1]}\n",
    "       \n",
    "\n",
    "f1_data = pd.DataFrame(data=data, index=index)\n",
    "\n",
    "f1_data.sort_values(by='F1 на валидационной выборке', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af23331",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Тестрирование лучшей модели по результататам ваолидационной выборки LogisticRegression с показателем метрики f1 0.781859\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1782f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = gscv_log.predict(features_test.text)\n",
    "lr_test_f1 = f1_score(target_test, predictions_test)\n",
    "print('финальный F1 логистической регрессии =', round(lr_test_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803b61e",
   "metadata": {},
   "source": [
    "# Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09904acd",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "В проекте:\n",
    "\n",
    "<li>загрузил данные и провел их предобработку - удаление лишних данных, очистку текстов, лемматизацию</li>\n",
    "<li>обучил 4 модели с разными гиперпараметрами и выбарл лучшую для тестрирования</li>\n",
    "<li>Провел тестрирование на LogisticRegression итоговый показатель метрики получился 0.76</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15dc966",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Аутсайдером среди моделей стали RandomForestClassifier и DecisionTreeClassifier, так как дали наименьшее F1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a074606",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Наилучшей моделью стала LogisticRegression, которая на тестировании показала F1 = 0.76. Поскольку требовалось найти модель классификации комментариев на позитивные и негативные со значением метрики качества F1 >= 0.75, рекомендовать могу LogisticRegression\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3,
    "start_time": "2024-06-03T21:49:31.350Z"
   },
   {
    "duration": 1331,
    "start_time": "2024-06-03T21:50:51.056Z"
   },
   {
    "duration": 890,
    "start_time": "2024-06-03T21:51:56.037Z"
   },
   {
    "duration": 302,
    "start_time": "2024-06-03T21:52:06.878Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-03T21:53:45.939Z"
   },
   {
    "duration": 4116,
    "start_time": "2024-06-03T21:53:46.487Z"
   },
   {
    "duration": 9,
    "start_time": "2024-06-03T21:53:54.669Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-03T21:54:10.077Z"
   },
   {
    "duration": 360,
    "start_time": "2024-06-03T21:54:21.275Z"
   },
   {
    "duration": 522,
    "start_time": "2024-06-03T21:54:49.687Z"
   },
   {
    "duration": 1147095,
    "start_time": "2024-06-03T21:54:54.359Z"
   },
   {
    "duration": 7,
    "start_time": "2024-06-03T22:14:53.047Z"
   },
   {
    "duration": 34,
    "start_time": "2024-06-03T22:14:55.170Z"
   },
   {
    "duration": 45,
    "start_time": "2024-06-03T22:15:26.772Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-03T22:15:37.489Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-03T22:16:00.572Z"
   },
   {
    "duration": 35,
    "start_time": "2024-06-03T22:16:11.696Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-03T22:17:53.223Z"
   },
   {
    "duration": 885705,
    "start_time": "2024-06-03T22:18:54.750Z"
   },
   {
    "duration": 81246,
    "start_time": "2024-06-03T22:33:40.456Z"
   },
   {
    "duration": 81,
    "start_time": "2024-06-03T22:40:24.196Z"
   },
   {
    "duration": 404800,
    "start_time": "2024-06-03T22:40:44.396Z"
   },
   {
    "duration": 413342,
    "start_time": "2024-06-03T22:47:29.198Z"
   },
   {
    "duration": 134598,
    "start_time": "2024-06-03T22:54:22.541Z"
   },
   {
    "duration": 59,
    "start_time": "2024-06-03T22:56:37.141Z"
   },
   {
    "duration": 10,
    "start_time": "2024-06-03T22:58:22.857Z"
   },
   {
    "duration": 1153,
    "start_time": "2024-06-03T22:59:36.233Z"
   },
   {
    "duration": 1199,
    "start_time": "2024-06-03T23:02:53.054Z"
   },
   {
    "duration": 1309,
    "start_time": "2024-06-03T23:17:55.163Z"
   },
   {
    "duration": 205,
    "start_time": "2024-06-03T23:17:56.474Z"
   },
   {
    "duration": 889,
    "start_time": "2024-06-03T23:17:57.401Z"
   },
   {
    "duration": 302,
    "start_time": "2024-06-03T23:17:58.292Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-03T23:17:58.595Z"
   },
   {
    "duration": 4112,
    "start_time": "2024-06-03T23:17:58.600Z"
   },
   {
    "duration": 7,
    "start_time": "2024-06-03T23:18:02.713Z"
   },
   {
    "duration": 92,
    "start_time": "2024-06-03T23:18:02.722Z"
   },
   {
    "duration": 1275,
    "start_time": "2024-06-03T23:18:12.068Z"
   },
   {
    "duration": 142,
    "start_time": "2024-06-03T23:18:13.620Z"
   },
   {
    "duration": 861,
    "start_time": "2024-06-03T23:18:14.636Z"
   },
   {
    "duration": 299,
    "start_time": "2024-06-03T23:18:15.499Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-03T23:18:15.799Z"
   },
   {
    "duration": 4054,
    "start_time": "2024-06-03T23:18:15.804Z"
   },
   {
    "duration": 6,
    "start_time": "2024-06-03T23:18:19.860Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-03T23:18:19.868Z"
   },
   {
    "duration": 1043372,
    "start_time": "2024-06-03T23:18:19.873Z"
   },
   {
    "duration": 7,
    "start_time": "2024-06-03T23:35:43.246Z"
   },
   {
    "duration": 51,
    "start_time": "2024-06-03T23:35:43.254Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-03T23:35:43.306Z"
   },
   {
    "duration": 31,
    "start_time": "2024-06-03T23:35:43.313Z"
   },
   {
    "duration": 44,
    "start_time": "2024-06-03T23:35:43.346Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-03T23:35:43.392Z"
   },
   {
    "duration": 897471,
    "start_time": "2024-06-03T23:35:43.395Z"
   },
   {
    "duration": 1205,
    "start_time": "2024-06-03T23:50:40.868Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
